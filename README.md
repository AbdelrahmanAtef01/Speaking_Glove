## Speaking Glove ğŸ¤–ğŸ§¤
A speaking glove project for the speechless that translates sign language into readable, listenable sequences based on AtMega32 and KNN Algorithm


## ğŸ‘¥ Contributors
| <img src="https://avatars.githubusercontent.com/u/109768834?s=400&u=b1286f34b7952b23ef706ef8d02e48fd0fb78751&v=4" width="150px;"/><br /><sub><b>[Abdelrahman Atef](https://github.com/AbdelrahmanAtef01)</b></sub><br /> | <img src="https://avatars.githubusercontent.com/u/136843019?v=4" width="150px;"/><br /><sub><b>[Hanin Sherif](https://github.com/HaninSh)</b></sub><br /> | <img src="https://avatars.githubusercontent.com/u/146020869?v=4" width="150px;"/><br /><sub><b>[Andrew Tanas](https://github.com/andrew-tanas)</b></sub><br /> |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |



---

## ğŸ“‘ Table of Contents
1. [Introduction](#introduction)
2. [ğŸ¥ Demo Video](#demo-video)
3. [âœ¨ Features](#features)
4. [ğŸ› ï¸ Dataset Preparation](#dataset-preparation)
5. [ğŸ”Œ Required Components](#required-components)
6. [âš™ï¸ System Architecture](#system-architecture)
   - [ğŸ“š Libraries](#libraries)
   - [ğŸ”‹ MCAL (Microcontroller Abstraction Layer)](#mcal)
   - [ğŸ–¥ï¸ HAL (Hardware Abstraction Layer)](#hal)
   - [ğŸ§© Services](#services)
7. [ğŸ¤ Connect & Collaborate](#connect--collaborate)

---

## ğŸ“ Introduction
The Speaking Glove is designed to improve the lives of people with speech disabilities. This is achieved by translating sign language into a text sequence, which is displayed on LCD screen, and spoken words displayed through an audio speaker. The glove interprets hand gestures and converts them to the corresponding text and audio using Embedded AI.

## ğŸ¥ Demo Video
[ğŸ”— Watch the demo video here](#)

## âœ¨ Features
- **Scalability** ğŸŒ: Unlike many similar projects limited to a simple set of predefined words, this project is highly scalable. We've integrated AI with embedded systems and implemented a simple K-Nearest Neighbors (KNN) algorithm. The KNN algorithm maps sensor readings to the closest matching word.
- **Customizable Vocabulary** ğŸ—£ï¸: To expand the vocabulary, you can simply add the sound files for new words to the M16P sound module and update the "sensor reads-to-word" dataset in the `get_word_sound` configuration file.
- **Sentence Handling** ğŸ“: The project can interpret sentences by detecting a **STOPPING_WORD** or reaching a maximum sentence length. Once a sentence is complete, it displays and resets for the next one.
- **Efficient Scheduling** â³: Although itâ€™s not built on an RTOS, the project schedules tasks using timers and flags, ensuring efficient parallel processing for reading and displaying without interference.

## ğŸ› ï¸ Dataset Preparation
To create or expand the dataset:
1. Change the main configuration to "test mode," which displays sensor readings on the LCD.
2. Perform the gesture for a new word and record the sensor readings.
3. Add these readings to the configuration file, update the total word count, and the system is ready with the new vocabulary.

## ğŸ”Œ Required Components
- Atmega32 microcontroller
- 5 Flex sensors
- 2 KY020 tilt sensors
- M16P sound module
- SD card
- Speaker
- LED
- 16x2 LCD
- 1k resistors
- 8M crystal oscillator
- 10nF capacitors
- Wiring

## âš™ï¸ System Architecture

### ğŸ“š Libraries
- **Standard Types** ğŸ“: Defines standard data types for consistency.
- **Bit Math** ğŸ”¢: Provides utilities for bit-level operations.

### ğŸ”‹ MCAL (Microcontroller Abstraction Layer)
- **DIO Driver**: Manages digital input/output, used by LCD and LED.
- **ADC (Analog-to-Digital Converter)**: Converts analog signals from flex sensors to digital values.
- **Global Interrupt**: Manages interrupt routines, used by timers and USART.
- **Timers**:
  - **Timer0** ğŸ•°ï¸: Manages display-related tasks.
  - **Timer1** â²ï¸: Manages sensor reading and conversion tasks.
- **USART** ğŸ”—: Enables UART communication, used to send commands to the M16P sound module.

### ğŸ–¥ï¸ HAL (Hardware Abstraction Layer)
- **LCD** ğŸ“º: Controls the LCD display for showing text.
- **LED** ğŸ’¡: Provides visual feedback, toggling when a reading is taken.
- **M16P Sound Module** ğŸ”Š: Plays audio for corresponding words.
- **Flex Sensor** âœ‹: Reads and converts sensor data, indicating each finger's position.
- **Tilt Sensor** ğŸ›ï¸: Detects hand angle to add contextual information.

### ğŸ§© Services
- **Get Glove Read** ğŸ“œ: Collects sensor readings in an array format.
- **Get Word Sound** ğŸ¤: Uses KNN to map sensor readings to the closest matching word.
- **Show** ğŸ–¥ï¸ğŸ”Š: Manages display and sound output, both on the LCD and through audio.

---

## ğŸ¤ Connect & Collaborate
Weâ€™d love for you to connect and collaborate on this project! Whether you're interested in contributing to the codebase or expanding the dataset, your help is welcome. 

ğŸ‘€ Feel free to reach out!
